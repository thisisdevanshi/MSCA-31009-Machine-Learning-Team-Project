{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9ylpCswTINb"
   },
   "source": [
    "# 03-Model: Bert\n",
    "\n",
    "**Topic:** Real or Not? NLP with Disaster Tweets\n",
    "<br>\n",
    "**Class:** MSCA 31009 Machine Learning \n",
    "<br>\n",
    "**Professor:** Dr. Arnab Bose\n",
    "<br>\n",
    "**Link:** https://www.kaggle.com/c/nlp-getting-started/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fZHxRj-TMog"
   },
   "source": [
    "# Why Bert?\n",
    "\n",
    "**Disadvantages of LSTM**\n",
    "<br>\n",
    "1. Slow to train. Words are passed in sequentially and generated sequentially \n",
    "<br>\n",
    "2. Not the best at capturing the true meaning of words (Even the bidrectional ones)\n",
    "\n",
    "**Transformer model** - This came out in a paper in 2017 titled, \"Attention is all you need\" which solves the above problem. They are **faster** as words can be used simultaneously and understand true contextual meaning as they are deeply bi-directional.\n",
    "\n",
    "<Br>\n",
    "\n",
    "Lets have a look at the architecture\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src =\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-01-32.png\">\n",
    "\n",
    "\n",
    "**Encoder** : Takes the words simultaneously and generates the embeddings simultaneously. Embeddings are vectors that encapsulate the meaning of the word. \n",
    "\n",
    "**Decoder** : Takes in the embeddings along with last outputs generated by the decoder model \n",
    "\n",
    "Since both of these parts learn some stuff individually they can be used indivually.  In case of english to french translation **Encoder** would learn What is english and what is contect. **Stacked Encoders =BERT**.  In the same example **Decoders** would learn how to map English to French words. Stacked Decoders = GPT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4Eb1e8-TQIq"
   },
   "source": [
    "# Understanding a Transformer\n",
    "\n",
    "- **Input Embedding** - We first input language data in form of emeddings i.e. numerical vectors that can encapsulate the meaning of the word. \n",
    "- **Positional Encoding** - Vectors that give context based on position of a word. They Sin/Cos for pos encoding\n",
    "- **Encoder Block**\n",
    "  - **Multi Head Attention**- Attention means which part should we focus on? So we are interested in knowing how any Ith word in the sentence is relevant to any other english word in the sentence. It is represnted in Ith attention vector. We find all the vectors and then take up weighted average because each word would give itself the highest attention. \n",
    "  - **Feed forward layer**-  We apply feed forward nets to all the attention vectors obtained above, also convert it to a shape accepted by the decoder block.\n",
    "- **Decoder Block**\n",
    "  - **Output Embedding** + **Positional Encoding**- We do the same thing. Convert outputs into embeddings and feed it to the decoder block \n",
    "  - **Multi Head Attention**- How much each word is related to other words in the embedding\n",
    "  - **Multi Head Attention/ Encoder Decoder Block** - Vectors from Encoders and Vectors from Output embedding are then passed into this block. This is where the mapping happens. For example: Each vector represents the relation between words in both input and output \n",
    "  - **Feed Forward** - Makes the output layer more digestable for linear layer\n",
    "- **Linear Layer** - Feed forward network that can convert the O/P into expected O/P length\n",
    "- **Softmax** - Gives the probability \n",
    "- **Final Word**- Word with highest probability\n",
    "\n",
    "\n",
    "**NOTE** - In the masked attention block for encoder I/P we use all the words in I/P whereas only previous words in O/P. So the matrix masks the next words to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HtlfW0-TTRm"
   },
   "source": [
    "# Limitation of a Transformer\n",
    "\n",
    "- can only deal with fixed-length text strings. The text has to be split into a certain number of segments or chunks before being fed into the system as input. This chunking of text causes context fragmentation.For example, if a sentence is split from the middle, then a significant amount of context is lost. In other words, the text is split without respecting the sentence or any other semantic boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10z7eoYQTamq"
   },
   "source": [
    "# What is Bert? \n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- Encoder blocks from the above architecture stack on top of each other\n",
    "\n",
    "- BERT is a deeply bidirectional model. Bidirectional means that BERT learns information from both the left and the right side of a token‚Äôs context during the training phase.\n",
    "\n",
    "- **Variants**\n",
    "  - BERT Base: 12 layers (transformer blocks), 12 attention heads, and 110 million parameters\n",
    "  - BERT Large: 24 layers (transformer blocks), 16 attention heads and, 340 million parameters\n",
    "\n",
    "\n",
    "# Text processing for Bert\n",
    "\n",
    "<img src= \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/bert_emnedding.png\">\n",
    "\n",
    "- **Token Embeddings:** These are the embeddings learned for the specific token from the WordPiece token vocabulary\n",
    "- **Segment Embeddings:** BERT can also take sentence pairs as inputs for tasks (Question-Answering). That‚Äôs why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. In the above example, all the tokens marked as EA belong to sentence A (and similarly for EB)\n",
    "- **Position Embeddings:** BERT learns and uses positional embeddings to express the position of words in a sentence. These are added to overcome the limitation of Transformer which, unlike an RNN, is not able to capture ‚Äúsequence‚Äù or ‚Äúorder‚Äù information\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW8c87oXTdzq"
   },
   "source": [
    "# Let's start with coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Zr77W-H6Tfgu"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "#libearies for DL\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "#!pip install transformers\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#libraries for data manipulation\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7R0TRXlUyOt",
    "outputId": "88dbe7c7-7c76-4ff1-fd2a-35e4cc2bf91b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Data_MSCA\n"
     ]
    }
   ],
   "source": [
    "#importing the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "%cd /content/drive/My Drive/Data_MSCA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zX0lS3J4VBY5"
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "4Y8SDsYeVF0Z",
    "outputId": "624881a9-f3e3-4bb9-a6a6-47d710e96cd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  ...                                               text target\n",
       "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
       "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
       "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
       "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
       "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets' have a look at the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "UAAb61cDVIgP",
    "outputId": "cadaa146-ccf3-4633-91c7-258fb2b1df48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6ze9sx9VL5z",
    "outputId": "3e967d6f-f8c6-4195-c24b-b225a599bdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#cleaning the dataset and getting in the form\n",
    "\n",
    "#removing the duplicates \n",
    "print(train_df.duplicated().sum())\n",
    "train_df = train_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v7KAD8aaVhoq"
   },
   "outputs": [],
   "source": [
    "#lower case all the text\n",
    "train_df['text']=train_df.apply(lambda row:row['text'].lower(),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:row['text'].lower(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5uyYvFmZVuBH"
   },
   "outputs": [],
   "source": [
    "#Removing the links from the text\n",
    "\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"//t.co/\\w+\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"//t.co/\\w+\",\"\",row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tKM0lm5yVzK4"
   },
   "outputs": [],
   "source": [
    "#for #\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"#\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"#\",\"\",row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UUTXjznjV1xM"
   },
   "outputs": [],
   "source": [
    "#for @\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"@\\w+\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"@\\w+\",\"\",row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lakPW5g8V3zL"
   },
   "outputs": [],
   "source": [
    "#Removing the HTML Tags\n",
    "\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"<.*?>\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"<.*?>\",\"\",row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5ooxoKn-V6ac"
   },
   "outputs": [],
   "source": [
    "#Removing URL's\n",
    "\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"https?://\\S+|www\\.\\S+\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"https?://\\S+|www\\.\\S+\",\"\",row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5QOOGftVV8QK"
   },
   "outputs": [],
   "source": [
    "#Removing Emoji's\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake üòîüòî \")\n",
    "train_df['text']=train_df['text'].apply(lambda x: remove_emoji(x))\n",
    "test_df['text']=test_df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ErPMrV6dWAOV"
   },
   "outputs": [],
   "source": [
    "#Removing line breaks and tabs\n",
    "\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"\\n\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"\\n\",\"\",row['text']),axis=1)\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"\\t\",\"\",row['text']),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"\\t\",\"\",row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q_7QkQylWCbL"
   },
   "outputs": [],
   "source": [
    "#Removing extra spaces\n",
    "\n",
    "train_df['text']=train_df.apply(lambda row:re.sub(r\"\\s\",\" \",row['text'].strip()),axis=1)\n",
    "test_df['text']=test_df.apply(lambda row:re.sub(r\"\\s\",\" \",row['text'].strip()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "IJS5YK0sWGni",
    "outputId": "4f64fdf8-d2d4-42bf-c06d-64d9f3fd707c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  ...                                               text target\n",
       "0   1     NaN  ...  our deeds are the reason of this earthquake ma...      1\n",
       "1   4     NaN  ...             forest fire near la ronge sask. canada      1\n",
       "2   5     NaN  ...  all residents asked to 'shelter in place' are ...      1\n",
       "3   6     NaN  ...  13,000 people receive wildfires evacuation ord...      1\n",
       "4   7     NaN  ...  just got sent this photo from ruby alaska as s...      1\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqIhfDUxddNv"
   },
   "source": [
    "# Let's understand the BERT Tokenizer\n",
    "\n",
    "BERT-Base, uncased uses a vocabulary of 30,522 words. The processes of tokenization involve splitting the input text into a list of tokens that are available in the vocabulary. In order to deal with the words not available in the vocabulary, BERT uses a technique called BPE based WordPiece tokenization. In this approach, an out of vocabulary word is progressively split into subwords and the word is then represented by a group of subwords. Since the subwords are part of the vocabulary, we have learned representations a context for these subwords and the context of the word is simply the combination of the context of the subwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yH_2BMBOeEgz",
    "outputId": "da0eaea6-9a06-4c3a-da60-d80e9af3ea55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes are: 2\n"
     ]
    }
   ],
   "source": [
    "#finding the number of class\n",
    "num_classes=len(train_df.target.unique())\n",
    "print(\"The number of classes are: {}\".format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RWXVlcf1ee8R"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81pXBZrwfEeS",
    "outputId": "cb98ce0e-d31c-442d-ab35-6679a735804e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences is: \n",
      "first night with retainers in. it's quite weird. better get used to it; i have to wear them every single night for the next year at least.\n",
      "['first', 'night', 'with', 'retain', '##ers', 'in', '.', 'it', \"'\", 's', 'quite', 'weird', '.', 'better', 'get', 'used', 'to', 'it', ';', 'i', 'have', 'to', 'wear', 'them', 'every', 'single', 'night', 'for', 'the', 'next', 'year', 'at', 'least', '.']\n"
     ]
    }
   ],
   "source": [
    "#looking at how Bert tokenizes the sentences\n",
    "print(\"Sentences is: \")\n",
    "print(train_df['text'][49])\n",
    "tokens=bert_tokenizer.tokenize(train_df['text'][49])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SryLmpYQh20_"
   },
   "source": [
    "As we can see the word retainers isn't here in the vocabulary and hence has been broken down into two words 'retain' and '##ers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIRA_PJ9ijjs"
   },
   "source": [
    "# Let's have a look at the parameters required for Bert\n",
    "\n",
    "\n",
    "1. **Input ID's**\n",
    "  - Input ID's : Index of input words in the BERT Vocabulary\n",
    "  - Batch Size: Number of examples\n",
    "  - Sequence Length: Number of tokens in sentence\n",
    "2. **Attention Masks**\n",
    "  - Mask to avoid performing attention on padding token indices. Mask values   selected in [0, 1]: 1 for tokens that are not masked, 0 for tokens that are marked (0 if the token is added by padding).\n",
    "\n",
    "3. **Label**\n",
    "  - Indices should be in [0, ..., num_classes- 1]. If num_classes == 1 a regression loss is computed (Mean-Square loss), If num_classes > 1 a classification loss is computed (Cross-Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "detxew0jl1Bh",
    "outputId": "1eb70da6-451a-43ee-dd22-d2d025439a4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2034, 2305, 2007, 9279, 2545, 1999, 1012, 2009, 1005, 1055, 3243, 6881, 1012, 2488, 2131, 2109, 2000, 2009, 1025, 1045, 2031, 2000, 4929, 2068, 2296, 2309, 2305, 2005, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sequence= bert_tokenizer.encode_plus(train_df['text'][49],add_special_tokens = True,max_length =30,pad_to_max_length = True, return_attention_mask = True)\n",
    "tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cgYRYNm0s7fI",
    "outputId": "29134477-ce7b-4e55-c9fc-81da61968c07"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] first night with retainers in. it's quite weird. better get used to it ; i have to wear them every single night for [SEP]\""
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see the decoded words\n",
    "bert_tokenizer.decode(tokenized_sequence['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLynu8NMtQP3"
   },
   "source": [
    "Special tokens \n",
    "- classifier [CLS] \n",
    "- separator [SEP]) \n",
    "- Padding [PAD] \n",
    "\n",
    "are added by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0zyLsTPfODgo"
   },
   "outputs": [],
   "source": [
    "#Let's load the sentences in the tokenizer\n",
    "def bert_encoder(dataset,max_l):\n",
    "  input_ids=[]\n",
    "  attention_masks=[]\n",
    "  for sent in dataset:\n",
    "     bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_l,pad_to_max_length = True,return_attention_mask = True)\n",
    "     input_ids.append(bert_inp['input_ids'])\n",
    "     attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "  input_ids=np.asarray(input_ids)\n",
    "  attention_masks=np.array(attention_masks)\n",
    "\n",
    "  return input_ids,attention_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kt0eFn2HOXsd",
    "outputId": "62950eb7-809d-407f-8113-7b4783213bb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_ids,train_masks=bert_encoder(train_df['text'],64)\n",
    "test_ids,test_masks=bert_encoder(test_df['text'],64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "cq1YuIc8YAdB"
   },
   "outputs": [],
   "source": [
    "def create_model(max_l):\n",
    "  bert_model = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "  input_ids = tf.keras.Input(shape=(max_l,),dtype='int32')\n",
    "  attention_masks = tf.keras.Input(shape=(max_l,),dtype='int32')\n",
    "  \n",
    "  output = bert_model([input_ids,attention_masks])\n",
    "  output = output[1]\n",
    "  output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
    "  output = tf.keras.layers.Dropout(0.2)(output)\n",
    "\n",
    "  output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "  model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bY07nPiYSMq",
    "outputId": "07828092-ce93-4309-f4ad-e86b873008ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 335141888   input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           32800       tf_bert_model_1[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_185 (Dropout)           (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dropout_185[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 335,174,721\n",
      "Trainable params: 335,174,721\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "il6THcxwYX9X",
    "outputId": "ffed52f5-1778-4b5b-bf67-2b01057655ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "381/381 [==============================] - 289s 759ms/step - loss: 0.3511 - accuracy: 0.8631 - val_loss: 0.3848 - val_accuracy: 0.8404\n",
      "Epoch 2/3\n",
      "381/381 [==============================] - 283s 743ms/step - loss: 0.3021 - accuracy: 0.8849 - val_loss: 0.4435 - val_accuracy: 0.8260\n",
      "Epoch 3/3\n",
      "381/381 [==============================] - 283s 742ms/step - loss: 0.2636 - accuracy: 0.8989 - val_loss: 0.3973 - val_accuracy: 0.8391\n"
     ]
    }
   ],
   "source": [
    "#creating checkpoint to save model\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True,save_weights_only=True)\n",
    "\n",
    "history = model.fit([train_ids,train_masks],train_df.target,validation_split=0.2, epochs=3,batch_size=16,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "T2A062gWYyLg"
   },
   "outputs": [],
   "source": [
    "#get the best weights\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "test_prediction = model.predict([test_ids,test_masks])\n",
    "submission['target'] = np.round(test_prediction).astype(int)\n",
    "submission.to_csv('submission_Trasnformers.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02-Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
