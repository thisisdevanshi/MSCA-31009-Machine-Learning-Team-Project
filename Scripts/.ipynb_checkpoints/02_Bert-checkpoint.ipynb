{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHmtarrQ1LqY"
   },
   "source": [
    "# 02-Model: Bert\n",
    "\n",
    "**Topic:** Real or Not? NLP with Disaster Tweets\n",
    "<br>\n",
    "**Class:** MSCA 31009 Machine Learning \n",
    "<br>\n",
    "**Professor:** Dr. Arnab Bose\n",
    "<br>\n",
    "**Link:** https://www.kaggle.com/c/nlp-getting-started/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjXRhg4K6X-t"
   },
   "source": [
    "# Why Bert?\n",
    "\n",
    "**Disadvantages of LSTM**\n",
    "<br>\n",
    "1. Slow to train. Words are passed in sequentially and generated sequentially \n",
    "<br>\n",
    "2. Not the best at capturing the true meaning of words (Even the bidrectional ones)\n",
    "\n",
    "**Transformer model** - This came out in a paper in 2017 titled, \"Attention is all you need\" which solves the above problem. They are **faster** as words can be used simultaneously and understand true contextual meaning as they are deeply bi-directional.\n",
    "\n",
    "<Br>\n",
    "\n",
    "Lets have a look at the architecture\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src =\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-01-32.png\">\n",
    "\n",
    "\n",
    "**Encoder** : Takes the words simultaneously and generates the embeddings simultaneously. Embeddings are vectors that encapsulate the meaning of the word. \n",
    "\n",
    "**Decoder** : Takes in the embeddings along with last outputs generated by the decoder model \n",
    "\n",
    "Since both of these parts learn some stuff individually they can be used indivually.  In case of english to french translation **Encoder** would learn What is english and what is contect. **Stacked Encoders =BERT**.  In the same example **Decoders** would learn how to map English to French words. Stacked Decoders = GPT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6ftfaLZFOyj"
   },
   "source": [
    "# Understanding a Transformer\n",
    "\n",
    "- **Input Embedding** - We first input language data in form of emeddings i.e. numerical vectors that can encapsulate the meaning of the word. \n",
    "- **Positional Encoding** - Vectors that give context based on position of a word. They Sin/Cos for pos encoding\n",
    "- **Encoder Block**\n",
    "  - **Multi Head Attention**- Attention means which part should we focus on? So we are interested in knowing how any Ith word in the sentence is relevant to any other english word in the sentence. It is represnted in Ith attention vector. We find all the vectors and then take up weighted average because each word would give itself the highest attention. \n",
    "  - **Feed forward layer**-  We apply feed forward nets to all the attention vectors obtained above, also convert it to a shape accepted by the decoder block.\n",
    "- **Decoder Block**\n",
    "  - **Output Embedding** + **Positional Encoding**- We do the same thing. Convert outputs into embeddings and feed it to the decoder block \n",
    "  - **Multi Head Attention**- How much each word is related to other words in the embedding\n",
    "  - **Multi Head Attention/ Encoder Decoder Block** - Vectors from Encoders and Vectors from Output embedding are then passed into this block. This is where the mapping happens. For example: Each vector represents the relation between words in both input and output \n",
    "  - **Feed Forward** - Makes the output layer more digestable for linear layer\n",
    "- **Linear Layer** - Feed forward network that can convert the O/P into expected O/P length\n",
    "- **Softmax** - Gives the probability \n",
    "- **Final Word**- Word with highest probability\n",
    "\n",
    "\n",
    "**NOTE** - In the masked attention block for encoder I/P we use all the words in I/P whereas only previous words in O/P. So the matrix masks the next words to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROR5tJ5qFNJh"
   },
   "source": [
    "# Limitation of a Transformer\n",
    "\n",
    "- can only deal with fixed-length text strings. The text has to be split into a certain number of segments or chunks before being fed into the system as input. This chunking of text causes context fragmentation.For example, if a sentence is split from the middle, then a significant amount of context is lost. In other words, the text is split without respecting the sentence or any other semantic boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty_4L7onbUDU"
   },
   "source": [
    "# What is Bert? \n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- Encoder blocks from the above architecture stack on top of each other\n",
    "\n",
    "- BERT is a deeply bidirectional model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase.\n",
    "\n",
    "- **Variants**\n",
    "  - BERT Base: 12 layers (transformer blocks), 12 attention heads, and 110 million parameters\n",
    "  - BERT Large: 24 layers (transformer blocks), 16 attention heads and, 340 million parameters\n",
    "\n",
    "\n",
    "# Text processing for Bert\n",
    "\n",
    "<img src= \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/bert_emnedding.png\">\n",
    "\n",
    "- **Token Embeddings:** These are the embeddings learned for the specific token from the WordPiece token vocabulary\n",
    "- **Segment Embeddings:** BERT can also take sentence pairs as inputs for tasks (Question-Answering). That’s why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. In the above example, all the tokens marked as EA belong to sentence A (and similarly for EB)\n",
    "- **Position Embeddings:** BERT learns and uses positional embeddings to express the position of words in a sentence. These are added to overcome the limitation of Transformer which, unlike an RNN, is not able to capture “sequence” or “order” information\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRuelLSJBKl5"
   },
   "source": [
    "# How to train Bert?\n",
    "\n",
    "- **Pretrain BERT** to understand language. \n",
    "\n",
    "**Goal** - *What is language? What is context?*\n",
    "\n",
    "1. **Masked Language Model** - Takes sentence with random sentences filled with Masks. It's like fill in the blanks to learn the language\n",
    "<br>\n",
    "<br>\n",
    "Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n",
    "<br>\n",
    "- Adding a classification layer on top of the encoder output.\n",
    "- Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
    "- Calculating the probability of each word in the vocabulary with softmax.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/986/0*ViwaI3Vvbnd-CJSQ.png\">\n",
    "\n",
    "The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models\n",
    "2. Next Sentence prediction -  Takes two sentences to see if the second sentence follows the first.\n",
    "<br>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1321/0*m_kXt3uqZH9e7H4w.png\">\n",
    "\n",
    "- A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "- A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "- A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GSpPnPcu5Fc"
   },
   "source": [
    "# Fine Tuning Bert\n",
    "\n",
    "Need to change thr O/P layer of model for respective tasks\n",
    "\n",
    "- **Classification** - Add softmax\n",
    "- **NER**- Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKxpIkeuv-px",
    "outputId": "57d10c8f-3b2c-4c82-fc78-aadb72d0e990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 8.9MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vYU4agQB_T16"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gfcnSP2f_cin"
   },
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EeOMF6fyAi6T"
   },
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRWUNTHyCeLC",
    "outputId": "31325a34-2345-49ee-d87b-1bb2641e7693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.7 s, sys: 11.9 s, total: 51.6 s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zW-kLSuzChOP",
    "outputId": "bfddefed-956f-4882-e7a6-c2d3d979ce95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Data_MSCA\n"
     ]
    }
   ],
   "source": [
    "#importing the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "%cd /content/drive/My Drive/Data_MSCA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0rdAi4NKD4ps"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SyNjAiRrEWsa"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TFouWjqkEYrs"
   },
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMXC9D0yEazA",
    "outputId": "2090b61c-7103-44a3-d70e-6397c0812660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BRuCZluEdXe",
    "outputId": "f1594cb5-2ab4-4960-c029-45bb7fb2bcf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "381/381 [==============================] - 761s 2s/step - loss: 0.1449 - accuracy: 0.9473 - val_loss: 0.5276 - val_accuracy: 0.8437\n",
      "Epoch 2/3\n",
      "381/381 [==============================] - 684s 2s/step - loss: 0.0851 - accuracy: 0.9693 - val_loss: 0.6027 - val_accuracy: 0.8510\n",
      "Epoch 3/3\n",
      "381/381 [==============================] - 684s 2s/step - loss: 0.0577 - accuracy: 0.9755 - val_loss: 0.6909 - val_accuracy: 0.8194\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fkzzaziyEizm"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FADTFPMtDXvL"
   },
   "outputs": [],
   "source": [
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission_colab.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02-Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
